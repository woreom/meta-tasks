{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woreom/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from generative_adversarial import Generator, Discriminator, GANSupervisor\n",
    "\n",
    "\n",
    "state = 42\n",
    "random.seed(state)\n",
    "np.random.seed(state)\n",
    "torch.manual_seed(state)\n",
    "torch.cuda.manual_seed(state)\n",
    "torch.backends.cudnn.enabled=False\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_dataset(path: str= \"../../datasets/mnist/\",\n",
    "                      mode: str= \"train\") -> torch.utils.data.Dataset:\n",
    "\n",
    "\n",
    "        # define transforms\n",
    "        transform = transforms.Compose([\n",
    "                transforms.Resize((64,64)),\n",
    "                transforms.CenterCrop((64,64)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ]) if mode != 'train' else transforms.Compose([\n",
    "              transforms.Resize((64,64)),\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.Resize((64,64)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ])\n",
    "        \n",
    "        dataset = MNIST(root= path, transform=transform,\n",
    "                train= False if mode == 'test' else True, download= False)\n",
    "        \n",
    "        # if mode != \"test\":\n",
    "        #     num_train = len(dataset)\n",
    "        #     num_val = int(np.floor(0.25 * num_train))\n",
    "\n",
    "        #     dataset= random_split(dataset=dataset, lengths=(num_train - num_val, num_val))[0 if mode == 'train' else 1]\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "def get_bird_dataset(path: str= \"../../datasets/birds/\",\n",
    "                        mode: str= \"train\") -> torch.utils.data.Dataset:\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Resize((64,64)),\n",
    "         transforms.Normalize((0.5), (0.5))\n",
    "        ])\n",
    "\n",
    "    # transform = transforms.ToTensor()\n",
    "\n",
    "    dataset = datasets.ImageFolder(path+f'/{mode}/',\n",
    "                                        transform=transform)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160: 100%|██████████| 276/276 [10:24<00:00,  2.26s/batch, D_loss=0.6438, G_loss=2.6479, fake_loss=0.383, real_loss=0.261]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Epoch 161: 100%|██████████| 276/276 [11:06<00:00,  2.42s/batch, D_loss=0.6732, G_loss=3.1424, fake_loss=0.372, real_loss=0.301]\n",
      "Epoch 162: 100%|██████████| 276/276 [09:48<00:00,  2.13s/batch, D_loss=0.7189, G_loss=3.2883, fake_loss=0.399, real_loss=0.320]\n",
      "Epoch 163: 100%|██████████| 276/276 [09:13<00:00,  2.01s/batch, D_loss=0.5335, G_loss=2.4279, fake_loss=0.291, real_loss=0.242]\n",
      "Epoch 164: 100%|██████████| 276/276 [10:51<00:00,  2.36s/batch, D_loss=0.6647, G_loss=3.0517, fake_loss=0.386, real_loss=0.279]\n",
      "Epoch 165: 100%|██████████| 276/276 [09:58<00:00,  2.17s/batch, D_loss=0.6973, G_loss=2.9496, fake_loss=0.410, real_loss=0.288]\n",
      "Epoch 166: 100%|██████████| 276/276 [08:43<00:00,  1.90s/batch, D_loss=0.5330, G_loss=2.3218, fake_loss=0.284, real_loss=0.249]\n",
      "Epoch 167: 100%|██████████| 276/276 [10:02<00:00,  2.18s/batch, D_loss=0.5293, G_loss=2.2361, fake_loss=0.292, real_loss=0.238]\n",
      "Epoch 168: 100%|██████████| 276/276 [10:15<00:00,  2.23s/batch, D_loss=0.5700, G_loss=2.3917, fake_loss=0.307, real_loss=0.263]\n",
      "Epoch 169: 100%|██████████| 276/276 [10:13<00:00,  2.22s/batch, D_loss=0.5268, G_loss=2.2898, fake_loss=0.284, real_loss=0.243]\n",
      "Epoch 170: 100%|██████████| 276/276 [10:08<00:00,  2.21s/batch, D_loss=0.5348, G_loss=2.2687, fake_loss=0.298, real_loss=0.237]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Epoch 171:  41%|████      | 113/276 [04:22<06:18,  2.32s/batch, D_loss=0.4847, G_loss=2.1215, fake_loss=0.253, real_loss=0.231]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m dis \u001b[39m=\u001b[39m  Discriminator(feature_size\u001b[39m=\u001b[39m\u001b[39m96\u001b[39m, num_channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m supervisor \u001b[39m=\u001b[39m GANSupervisor(generator\u001b[39m=\u001b[39m gen,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m               discriminator\u001b[39m=\u001b[39m dis,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m               generator_optimizer\u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m               embedding_size\u001b[39m=\u001b[39m \u001b[39m100\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m               )\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m supervisor\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[0;32m/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.py:206\u001b[0m, in \u001b[0;36mGANSupervisor.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m X \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m G_losses[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m D_losses[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.5\u001b[39m:\n\u001b[0;32m--> 206\u001b[0m     G_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_generator(X)\n\u001b[1;32m    208\u001b[0m \u001b[39melif\u001b[39;00m G_losses[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m D_losses[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1.5\u001b[39m:\n\u001b[1;32m    209\u001b[0m     D_loss, real_loss, fake_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_discriminator(X)\n",
      "File \u001b[0;32m/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.py:146\u001b[0m, in \u001b[0;36mGANSupervisor.train_generator\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    144\u001b[0m G_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(D_fake, ones_label)\n\u001b[1;32m    145\u001b[0m \u001b[39m# Calculate gradients for G\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m G_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    147\u001b[0m \u001b[39m# Update G\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen_optim\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/few/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/few/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "gen =  Generator(input_size=100, feature_size=96, num_channels=3)\n",
    "dis =  Discriminator(feature_size=96, num_channels=3,)\n",
    "\n",
    "supervisor = GANSupervisor(generator= gen,\n",
    "              discriminator= dis,\n",
    "              generator_optimizer= optim.Adam,\n",
    "              discriminator_optimizer= optim.Adam,\n",
    "              generator_learning_rate= 1e-5,\n",
    "              discriminator_learning_rate=1e-5 ,\n",
    "              get_dataset= get_bird_dataset,\n",
    "              get_noise_generator=None,\n",
    "              epoch= 1000,\n",
    "              batch_size= 256,\n",
    "              embedding_size= 100,\n",
    "              )\n",
    "\n",
    "supervisor.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 276/276 [21:46<00:00,  4.73s/batch, D_loss=2.7721, G_loss=2.7212, fake_loss=0.069, real_loss=2.703]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Epoch 1: 100%|██████████| 276/276 [19:41<00:00,  4.28s/batch, D_loss=2.1871, G_loss=2.1338, fake_loss=0.125, real_loss=2.062]\n",
      "Epoch 2: 100%|██████████| 276/276 [19:07<00:00,  4.16s/batch, D_loss=1.8844, G_loss=1.8309, fake_loss=0.173, real_loss=1.711]\n",
      "Epoch 3: 100%|██████████| 276/276 [18:59<00:00,  4.13s/batch, D_loss=1.6916, G_loss=1.5997, fake_loss=0.224, real_loss=1.468]\n",
      "Epoch 4: 100%|██████████| 276/276 [18:33<00:00,  4.04s/batch, D_loss=1.5536, G_loss=1.4150, fake_loss=0.276, real_loss=1.278]\n",
      "Epoch 5: 100%|██████████| 276/276 [18:53<00:00,  4.11s/batch, D_loss=1.4518, G_loss=1.2640, fake_loss=0.329, real_loss=1.123]\n",
      "Epoch 6: 100%|██████████| 276/276 [18:34<00:00,  4.04s/batch, D_loss=1.3758, G_loss=1.1413, fake_loss=0.382, real_loss=0.994]\n",
      "Epoch 7: 100%|██████████| 276/276 [18:46<00:00,  4.08s/batch, D_loss=1.3185, G_loss=1.0439, fake_loss=0.430, real_loss=0.888]\n",
      "Epoch 8: 100%|██████████| 276/276 [18:31<00:00,  4.03s/batch, D_loss=1.2744, G_loss=0.9689, fake_loss=0.473, real_loss=0.801]\n",
      "Epoch 9: 100%|██████████| 276/276 [18:10<00:00,  3.95s/batch, D_loss=1.2363, G_loss=0.9166, fake_loss=0.507, real_loss=0.730]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Epoch 10: 100%|██████████| 276/276 [11:50<00:00,  2.57s/batch, D_loss=1.1548, G_loss=0.9111, fake_loss=0.510, real_loss=0.644]\n",
      "Epoch 11: 100%|██████████| 276/276 [11:40<00:00,  2.54s/batch, D_loss=1.0861, G_loss=0.9204, fake_loss=0.504, real_loss=0.582]\n",
      "Epoch 12: 100%|██████████| 276/276 [11:43<00:00,  2.55s/batch, D_loss=1.0229, G_loss=0.9438, fake_loss=0.489, real_loss=0.534]\n",
      "Epoch 13: 100%|██████████| 276/276 [11:36<00:00,  2.52s/batch, D_loss=0.9583, G_loss=0.9807, fake_loss=0.466, real_loss=0.492]\n",
      "Epoch 14: 100%|██████████| 276/276 [11:53<00:00,  2.59s/batch, D_loss=0.8784, G_loss=1.0336, fake_loss=0.436, real_loss=0.442]\n",
      "Epoch 15: 100%|██████████| 276/276 [11:56<00:00,  2.60s/batch, D_loss=0.7725, G_loss=1.1135, fake_loss=0.395, real_loss=0.377]\n",
      "Epoch 16: 100%|██████████| 276/276 [11:47<00:00,  2.56s/batch, D_loss=0.6786, G_loss=1.2106, fake_loss=0.351, real_loss=0.327]\n",
      "Epoch 17: 100%|██████████| 276/276 [11:47<00:00,  2.57s/batch, D_loss=0.5873, G_loss=1.3255, fake_loss=0.306, real_loss=0.281]\n",
      "Epoch 18: 100%|██████████| 276/276 [12:00<00:00,  2.61s/batch, D_loss=0.4943, G_loss=1.4721, fake_loss=0.259, real_loss=0.236]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Epoch 19: 100%|██████████| 276/276 [11:57<00:00,  2.60s/batch, D_loss=0.3998, G_loss=1.6632, fake_loss=0.209, real_loss=0.191]\n",
      "Epoch 20: 100%|██████████| 276/276 [09:20<00:00,  2.03s/batch, D_loss=0.3349, G_loss=1.8232, fake_loss=0.174, real_loss=0.160]\n",
      "Epoch 21: 100%|██████████| 276/276 [09:05<00:00,  1.98s/batch, D_loss=0.3331, G_loss=1.8302, fake_loss=0.173, real_loss=0.160]\n",
      "Epoch 22: 100%|██████████| 276/276 [12:33<00:00,  2.73s/batch, D_loss=0.3339, G_loss=1.8312, fake_loss=0.173, real_loss=0.161]\n",
      "Epoch 23: 100%|██████████| 276/276 [12:19<00:00,  2.68s/batch, D_loss=0.3325, G_loss=1.8322, fake_loss=0.173, real_loss=0.160]\n",
      "Epoch 24: 100%|██████████| 276/276 [12:15<00:00,  2.67s/batch, D_loss=0.3324, G_loss=1.8329, fake_loss=0.173, real_loss=0.160]\n",
      "Epoch 25: 100%|██████████| 276/276 [12:22<00:00,  2.69s/batch, D_loss=0.3322, G_loss=1.8341, fake_loss=0.172, real_loss=0.160]\n",
      "Epoch 26: 100%|██████████| 276/276 [11:51<00:00,  2.58s/batch, D_loss=0.3321, G_loss=1.8358, fake_loss=0.172, real_loss=0.160]\n",
      "Epoch 27: 100%|██████████| 276/276 [11:50<00:00,  2.57s/batch, D_loss=0.3303, G_loss=1.8370, fake_loss=0.172, real_loss=0.158]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Epoch 28: 100%|██████████| 276/276 [12:05<00:00,  2.63s/batch, D_loss=0.3296, G_loss=1.8380, fake_loss=0.172, real_loss=0.158]\n",
      "Epoch 29: 100%|██████████| 276/276 [12:19<00:00,  2.68s/batch, D_loss=0.3311, G_loss=1.8393, fake_loss=0.171, real_loss=0.160]\n",
      "Epoch 30: 100%|██████████| 276/276 [12:09<00:00,  2.64s/batch, D_loss=0.3295, G_loss=1.8400, fake_loss=0.171, real_loss=0.158]\n",
      "Epoch 31: 100%|██████████| 276/276 [12:00<00:00,  2.61s/batch, D_loss=0.3318, G_loss=1.8410, fake_loss=0.171, real_loss=0.161]\n",
      "Epoch 32: 100%|██████████| 276/276 [12:11<00:00,  2.65s/batch, D_loss=0.3286, G_loss=1.8421, fake_loss=0.171, real_loss=0.158]\n",
      "Epoch 33: 100%|██████████| 276/276 [12:22<00:00,  2.69s/batch, D_loss=0.3299, G_loss=1.8421, fake_loss=0.171, real_loss=0.159]\n",
      "Epoch 34: 100%|██████████| 276/276 [12:19<00:00,  2.68s/batch, D_loss=0.3293, G_loss=1.8438, fake_loss=0.171, real_loss=0.159]\n",
      "Epoch 35: 100%|██████████| 276/276 [11:49<00:00,  2.57s/batch, D_loss=0.3283, G_loss=1.8440, fake_loss=0.170, real_loss=0.158]\n",
      "Epoch 36: 100%|██████████| 276/276 [12:01<00:00,  2.61s/batch, D_loss=0.3277, G_loss=1.8436, fake_loss=0.170, real_loss=0.157]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Epoch 37: 100%|██████████| 276/276 [12:08<00:00,  2.64s/batch, D_loss=0.3287, G_loss=1.8453, fake_loss=0.170, real_loss=0.158]\n",
      "Epoch 38: 100%|██████████| 276/276 [12:11<00:00,  2.65s/batch, D_loss=0.3277, G_loss=1.8460, fake_loss=0.170, real_loss=0.158]\n",
      "Epoch 39: 100%|██████████| 276/276 [12:23<00:00,  2.69s/batch, D_loss=0.3271, G_loss=1.8476, fake_loss=0.170, real_loss=0.157]\n",
      "Epoch 40: 100%|██████████| 276/276 [13:59<00:00,  3.04s/batch, D_loss=0.3265, G_loss=1.8485, fake_loss=0.170, real_loss=0.157]\n",
      "Epoch 41: 100%|██████████| 276/276 [12:47<00:00,  2.78s/batch, D_loss=0.3269, G_loss=1.8477, fake_loss=0.170, real_loss=0.157]\n",
      "Epoch 42: 100%|██████████| 276/276 [12:11<00:00,  2.65s/batch, D_loss=0.3261, G_loss=1.8490, fake_loss=0.170, real_loss=0.156]\n",
      "Epoch 43: 100%|██████████| 276/276 [11:42<00:00,  2.54s/batch, D_loss=0.3276, G_loss=1.8519, fake_loss=0.169, real_loss=0.158]\n",
      "Epoch 44: 100%|██████████| 276/276 [09:04<00:00,  1.97s/batch, D_loss=0.3260, G_loss=1.8522, fake_loss=0.169, real_loss=0.157]\n",
      "Epoch 45: 100%|██████████| 276/276 [08:49<00:00,  1.92s/batch, D_loss=0.3248, G_loss=1.8539, fake_loss=0.169, real_loss=0.156]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Epoch 46: 100%|██████████| 276/276 [09:12<00:00,  2.00s/batch, D_loss=0.3247, G_loss=1.8562, fake_loss=0.169, real_loss=0.156]\n",
      "Epoch 47: 100%|██████████| 276/276 [08:54<00:00,  1.94s/batch, D_loss=0.3238, G_loss=1.8575, fake_loss=0.168, real_loss=0.156]\n",
      "Epoch 48: 100%|██████████| 276/276 [08:45<00:00,  1.91s/batch, D_loss=0.3232, G_loss=1.8584, fake_loss=0.168, real_loss=0.155]\n",
      "Epoch 49: 100%|██████████| 276/276 [08:47<00:00,  1.91s/batch, D_loss=0.3236, G_loss=1.8595, fake_loss=0.168, real_loss=0.156]\n",
      "Epoch 50: 100%|██████████| 276/276 [08:45<00:00,  1.90s/batch, D_loss=0.3241, G_loss=1.8602, fake_loss=0.167, real_loss=0.157]\n",
      "Epoch 51: 100%|██████████| 276/276 [08:38<00:00,  1.88s/batch, D_loss=0.3218, G_loss=1.8604, fake_loss=0.167, real_loss=0.154]\n",
      "Epoch 52: 100%|██████████| 276/276 [08:48<00:00,  1.92s/batch, D_loss=0.3220, G_loss=1.8592, fake_loss=0.167, real_loss=0.155]\n",
      "Epoch 53: 100%|██████████| 276/276 [08:55<00:00,  1.94s/batch, D_loss=0.3222, G_loss=1.8610, fake_loss=0.167, real_loss=0.155]\n",
      "Epoch 54: 100%|██████████| 276/276 [08:57<00:00,  1.95s/batch, D_loss=0.3217, G_loss=1.8617, fake_loss=0.167, real_loss=0.155]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Epoch 55: 100%|██████████| 276/276 [08:50<00:00,  1.92s/batch, D_loss=0.3216, G_loss=1.8643, fake_loss=0.167, real_loss=0.155]\n",
      "Epoch 56: 100%|██████████| 276/276 [08:51<00:00,  1.92s/batch, D_loss=0.3228, G_loss=1.8663, fake_loss=0.167, real_loss=0.156]\n",
      "Epoch 57: 100%|██████████| 276/276 [08:46<00:00,  1.91s/batch, D_loss=0.3203, G_loss=1.8673, fake_loss=0.166, real_loss=0.154]\n",
      "Epoch 58: 100%|██████████| 276/276 [08:58<00:00,  1.95s/batch, D_loss=0.3195, G_loss=1.8674, fake_loss=0.166, real_loss=0.153]\n",
      "Epoch 59: 100%|██████████| 276/276 [09:13<00:00,  2.01s/batch, D_loss=0.3203, G_loss=1.8690, fake_loss=0.166, real_loss=0.154]\n",
      "Epoch 60: 100%|██████████| 276/276 [08:55<00:00,  1.94s/batch, D_loss=0.3195, G_loss=1.8702, fake_loss=0.166, real_loss=0.154]\n",
      "Epoch 61: 100%|██████████| 276/276 [08:48<00:00,  1.92s/batch, D_loss=0.3193, G_loss=1.8711, fake_loss=0.166, real_loss=0.154]\n",
      "Epoch 62: 100%|██████████| 276/276 [08:44<00:00,  1.90s/batch, D_loss=0.3193, G_loss=1.8714, fake_loss=0.165, real_loss=0.154]\n",
      "Epoch 63: 100%|██████████| 276/276 [08:45<00:00,  1.90s/batch, D_loss=0.3186, G_loss=1.8695, fake_loss=0.165, real_loss=0.153]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Epoch 64: 100%|██████████| 276/276 [08:53<00:00,  1.93s/batch, D_loss=0.3199, G_loss=1.8727, fake_loss=0.166, real_loss=0.154]\n",
      "Epoch 65: 100%|██████████| 276/276 [08:59<00:00,  1.96s/batch, D_loss=0.3177, G_loss=1.8726, fake_loss=0.165, real_loss=0.153]\n",
      "Epoch 66: 100%|██████████| 276/276 [09:12<00:00,  2.00s/batch, D_loss=0.3174, G_loss=1.8755, fake_loss=0.165, real_loss=0.152]\n",
      "Epoch 67: 100%|██████████| 276/276 [09:03<00:00,  1.97s/batch, D_loss=0.3175, G_loss=1.8772, fake_loss=0.165, real_loss=0.153]\n",
      "Epoch 68: 100%|██████████| 276/276 [09:05<00:00,  1.98s/batch, D_loss=0.3212, G_loss=1.8785, fake_loss=0.164, real_loss=0.157]\n",
      "Epoch 69: 100%|██████████| 276/276 [08:55<00:00,  1.94s/batch, D_loss=0.3176, G_loss=1.8797, fake_loss=0.164, real_loss=0.154]\n",
      "Epoch 70: 100%|██████████| 276/276 [08:49<00:00,  1.92s/batch, D_loss=0.3171, G_loss=1.8792, fake_loss=0.164, real_loss=0.153]\n",
      "Epoch 71: 100%|██████████| 276/276 [09:04<00:00,  1.97s/batch, D_loss=0.3164, G_loss=1.8816, fake_loss=0.164, real_loss=0.153]\n",
      "Epoch 72: 100%|██████████| 276/276 [09:18<00:00,  2.02s/batch, D_loss=0.3161, G_loss=1.8814, fake_loss=0.164, real_loss=0.153]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Epoch 73: 100%|██████████| 276/276 [09:12<00:00,  2.00s/batch, D_loss=0.3173, G_loss=1.8792, fake_loss=0.164, real_loss=0.154]\n",
      "Epoch 74: 100%|██████████| 276/276 [09:04<00:00,  1.97s/batch, D_loss=0.3166, G_loss=1.8753, fake_loss=0.164, real_loss=0.153]\n",
      "Epoch 75: 100%|██████████| 276/276 [09:06<00:00,  1.98s/batch, D_loss=0.3183, G_loss=1.8827, fake_loss=0.165, real_loss=0.154]\n",
      "Epoch 76: 100%|██████████| 276/276 [09:02<00:00,  1.96s/batch, D_loss=0.3141, G_loss=1.8867, fake_loss=0.163, real_loss=0.151]\n",
      "Epoch 77: 100%|██████████| 276/276 [09:04<00:00,  1.97s/batch, D_loss=0.3161, G_loss=1.8840, fake_loss=0.163, real_loss=0.153]\n",
      "Epoch 78: 100%|██████████| 276/276 [09:22<00:00,  2.04s/batch, D_loss=0.3136, G_loss=1.8805, fake_loss=0.163, real_loss=0.151]\n",
      "Epoch 79: 100%|██████████| 276/276 [09:20<00:00,  2.03s/batch, D_loss=0.3146, G_loss=1.8850, fake_loss=0.164, real_loss=0.151]\n",
      "Epoch 80: 100%|██████████| 276/276 [09:17<00:00,  2.02s/batch, D_loss=0.3157, G_loss=1.8880, fake_loss=0.163, real_loss=0.153]\n",
      "Epoch 81: 100%|██████████| 276/276 [09:09<00:00,  1.99s/batch, D_loss=0.3129, G_loss=1.8727, fake_loss=0.162, real_loss=0.151]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Epoch 82:   0%|          | 0/276 [00:04<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m supervisor \u001b[39m=\u001b[39m GANSupervisor(generator\u001b[39m=\u001b[39m gen,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m               discriminator\u001b[39m=\u001b[39m dis,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m               generator_optimizer\u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m               embedding_size\u001b[39m=\u001b[39m \u001b[39m100\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m               )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m supervisor\u001b[39m.\u001b[39mgen \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mout2/gen/gen9.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m supervisor\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[0;32m/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.py:212\u001b[0m, in \u001b[0;36mGANSupervisor.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m     D_loss, real_loss, fake_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_discriminator(X)\n\u001b[1;32m    211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     D_loss, real_loss, fake_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_discriminator(X)\n\u001b[1;32m    213\u001b[0m     G_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_generator(X)\n\u001b[1;32m    215\u001b[0m G_losses\u001b[39m.\u001b[39mappend(G_loss)\n",
      "File \u001b[0;32m/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.py:161\u001b[0m, in \u001b[0;36mGANSupervisor.train_discriminator\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    159\u001b[0m ones_label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((batch_size,), \u001b[39m1.\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    160\u001b[0m \u001b[39m# Forward pass real batch through D\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m D_real \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdis(X)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[39m# Calculate loss on all-real batch\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m# print(f\"D_real: {D_real.shape}\", f\"ones_label: {ones_label.shape}\")\u001b[39;00m\n\u001b[1;32m    164\u001b[0m D_loss_real \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(D_real, ones_label)\n",
      "File \u001b[0;32m~/anaconda3/envs/few/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb Cell 6\u001b[0m in \u001b[0;36mResDis.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavpool(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/generative_adversarial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/few/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/../prototypical_networks_with_autoencoders/autoencoders.py:95\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[1;32m     94\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n\u001b[0;32m---> 95\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(x)\n\u001b[1;32m     96\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv4(x)\n\u001b[1;32m     97\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv5(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/few/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/../prototypical_networks_with_autoencoders/autoencoders.py:187\u001b[0m, in \u001b[0;36mEncoderResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    185\u001b[0m     \u001b[39mfor\u001b[39;00m name, layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_children():\n\u001b[0;32m--> 187\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m    189\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/few/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/d/M3/Projects/OCAST/meta-learning/generative_adversarial_networks/../prototypical_networks_with_autoencoders/autoencoders.py:330\u001b[0m, in \u001b[0;36mEncoderResidualLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    327\u001b[0m identity \u001b[39m=\u001b[39m x\n\u001b[1;32m    329\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_layer1(x)\n\u001b[0;32m--> 330\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_layer2(x)\n\u001b[1;32m    332\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     identity \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample(identity)\n",
      "File \u001b[0;32m~/anaconda3/envs/few/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/few/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/few/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/few/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/few/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../prototypical_networks_with_autoencoders/')\n",
    "\n",
    "class ResDis(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(ResDis, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.avpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(in_features=512, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        x = self.avpool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "autoencoder = torch.load(\"../outputs/exported/resnet_autoencoder/autoencoder_withouthist.pt\")\n",
    "gen =  Generator(input_size=100, feature_size=96, num_channels=3)\n",
    "dis =  ResDis(encoder=autoencoder.encoder)\n",
    "\n",
    "autoencoder.encoder = torch.load(\"out2/dis/dis9.pt\")\n",
    "\n",
    "supervisor = GANSupervisor(generator= gen,\n",
    "              discriminator= dis,\n",
    "              generator_optimizer= optim.Adam,\n",
    "              discriminator_optimizer= optim.Adam,\n",
    "              generator_learning_rate= 1e-5,\n",
    "              discriminator_learning_rate=1e-5 ,\n",
    "              get_dataset= get_bird_dataset,\n",
    "              get_noise_generator=None,\n",
    "              epoch= 990,\n",
    "              batch_size= 256,\n",
    "              embedding_size= 100,\n",
    "              )\n",
    "\n",
    "supervisor.gen = torch.load(\"out2/gen/gen9.pt\")\n",
    "\n",
    "\n",
    "supervisor.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few",
   "language": "python",
   "name": "few"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e77f8d256905f3a55fdc1781e3f8cf4d8ee1796048327c8e58a3ee5cba17fb5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
